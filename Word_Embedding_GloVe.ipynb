{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter search with word embeddings\n",
    "#### Your task is to create a program that searches through twitter tweets using word embeddings. Given a search query, your program should return the top 5 tweets relating to this query for each distance algorithm used (you will use 2 distance algorithms, which means you'll return 10 tweets as a result. lore information below). You can achieve this by performing the following steps:\n",
    "\n",
    "### 1- Perform the necessary pre-processing on the tweets. Meep in mind that tweets contain lots of typos and non-conventional characters (like emoticons and the like)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation # to remove punctuation from corpus\n",
    "from nltk import pos_tag\n",
    "\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>849636868052275200</td>\n",
       "      <td>2017-04-05 14:56:29</td>\n",
       "      <td>b'And so the robots spared humanity ... https:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>848988730585096192</td>\n",
       "      <td>2017-04-03 20:01:01</td>\n",
       "      <td>b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>848943072423497728</td>\n",
       "      <td>2017-04-03 16:59:35</td>\n",
       "      <td>b'@waltmossberg @mims @defcon_5 Et tu, Walt?'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>848935705057280001</td>\n",
       "      <td>2017-04-03 16:30:19</td>\n",
       "      <td>b'Stormy weather in Shortville ...'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>848416049573658624</td>\n",
       "      <td>2017-04-02 06:05:23</td>\n",
       "      <td>b\"@DaveLeeBBC @verge Coal is dying due to nat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2814</th>\n",
       "      <td>142881284019060736</td>\n",
       "      <td>2011-12-03 08:22:07</td>\n",
       "      <td>b'That was a total non sequitur btw'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2815</th>\n",
       "      <td>142880871391838208</td>\n",
       "      <td>2011-12-03 08:20:28</td>\n",
       "      <td>b'Great Voltaire quote, arguably better than T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2816</th>\n",
       "      <td>142188458125963264</td>\n",
       "      <td>2011-12-01 10:29:04</td>\n",
       "      <td>b'I made the volume on the Model S http://t.co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2817</th>\n",
       "      <td>142179928203460608</td>\n",
       "      <td>2011-12-01 09:55:11</td>\n",
       "      <td>b\"Went to Iceland on Sat to ride bumper cars o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2818</th>\n",
       "      <td>15434727182</td>\n",
       "      <td>2010-06-04 18:31:57</td>\n",
       "      <td>b'Please ignore prior tweets, as that was some...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2819 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id              created_at                             text                       \n",
       "0     849636868052275200  2017-04-05 14:56:29  b'And so the robots spared humanity ... https:...\n",
       "1     848988730585096192  2017-04-03 20:01:01  b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exa...\n",
       "2     848943072423497728  2017-04-03 16:59:35      b'@waltmossberg @mims @defcon_5 Et tu, Walt?'\n",
       "3     848935705057280001  2017-04-03 16:30:19                b'Stormy weather in Shortville ...'\n",
       "4     848416049573658624  2017-04-02 06:05:23  b\"@DaveLeeBBC @verge Coal is dying due to nat ...\n",
       "...                  ...                  ...                                                ...\n",
       "2814  142881284019060736  2011-12-03 08:22:07               b'That was a total non sequitur btw'\n",
       "2815  142880871391838208  2011-12-03 08:20:28  b'Great Voltaire quote, arguably better than T...\n",
       "2816  142188458125963264  2011-12-01 10:29:04  b'I made the volume on the Model S http://t.co...\n",
       "2817  142179928203460608  2011-12-01 09:55:11  b\"Went to Iceland on Sat to ride bumper cars o...\n",
       "2818         15434727182  2010-06-04 18:31:57  b'Please ignore prior tweets, as that was some...\n",
       "\n",
       "[2819 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import dataset \n",
    "data = pd.read_csv('tweets.csv', encoding='utf-8')\n",
    "# We use the Utf-8 encoding to include emoticons\n",
    "\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>stop</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>849636868052275200</td>\n",
       "      <td>2017-04-05 14:56:29</td>\n",
       "      <td>so the robots spared humanity ...</td>\n",
       "      <td>robots spared humanity ...</td>\n",
       "      <td>[robot, spare, human]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>848988730585096192</td>\n",
       "      <td>2017-04-03 20:01:01</td>\n",
       "      <td>b\"    Exactly. Tesla is absurdly overvalued if...</td>\n",
       "      <td>b\" exactly. tesla absurdly overvalued based pa...</td>\n",
       "      <td>[exactli, tesla, absurdli, overvalu, base, pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>848943072423497728</td>\n",
       "      <td>2017-04-03 16:59:35</td>\n",
       "      <td>b'   Et tu, Walt?'</td>\n",
       "      <td>b' et tu, walt?'</td>\n",
       "      <td>[walt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>848935705057280001</td>\n",
       "      <td>2017-04-03 16:30:19</td>\n",
       "      <td>weather in Shortville ...'</td>\n",
       "      <td>weather shortville ...'</td>\n",
       "      <td>[weather, shortvil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>848416049573658624</td>\n",
       "      <td>2017-04-02 06:05:23</td>\n",
       "      <td>b\"  Coal is dying due to nat gas fracking. It'...</td>\n",
       "      <td>b\" coal dying nat gas fracking. it's basically...</td>\n",
       "      <td>[coal, dy, nat, ga, frack, basic, dead]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2814</th>\n",
       "      <td>142881284019060736</td>\n",
       "      <td>2011-12-03 08:22:07</td>\n",
       "      <td>was a total non sequitur btw'</td>\n",
       "      <td>total non sequitur btw'</td>\n",
       "      <td>[total, non, sequitur, btw]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2815</th>\n",
       "      <td>142880871391838208</td>\n",
       "      <td>2011-12-03 08:20:28</td>\n",
       "      <td>Voltaire quote, arguably better than Twain. H...</td>\n",
       "      <td>voltaire quote, arguably better twain. hearing...</td>\n",
       "      <td>[voltair, quot, arguabl, better, twain, hear, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2816</th>\n",
       "      <td>142188458125963264</td>\n",
       "      <td>2011-12-01 10:29:04</td>\n",
       "      <td>made the volume on the Model S  go to 11.  No...</td>\n",
       "      <td>volume model s 11. need work miniature stonehe...</td>\n",
       "      <td>[volum, model, need, work, miniatur, stoneheng]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2817</th>\n",
       "      <td>142179928203460608</td>\n",
       "      <td>2011-12-01 09:55:11</td>\n",
       "      <td>to Iceland on Sat to ride bumper cars on ice!...</td>\n",
       "      <td>iceland sat ride bumper cars ice! no, country,...</td>\n",
       "      <td>[iceland, sat, ride, bumper, car, ic, countri,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2818</th>\n",
       "      <td>15434727182</td>\n",
       "      <td>2010-06-04 18:31:57</td>\n",
       "      <td>ignore prior tweets, as that was someone pret...</td>\n",
       "      <td>ignore prior tweets, pretending :) actually me.'</td>\n",
       "      <td>[ignor, prior, tweet, pretend, actual]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2819 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id              created_at                             text                                               stop                                            tokenized                     \n",
       "0     849636868052275200  2017-04-05 14:56:29                 so the robots spared humanity ...                          robots spared humanity ...                              [robot, spare, human]\n",
       "1     848988730585096192  2017-04-03 20:01:01  b\"    Exactly. Tesla is absurdly overvalued if...  b\" exactly. tesla absurdly overvalued based pa...  [exactli, tesla, absurdli, overvalu, base, pas...\n",
       "2     848943072423497728  2017-04-03 16:59:35                                 b'   Et tu, Walt?'                                   b' et tu, walt?'                                             [walt]\n",
       "3     848935705057280001  2017-04-03 16:30:19                         weather in Shortville ...'                            weather shortville ...'                                [weather, shortvil]\n",
       "4     848416049573658624  2017-04-02 06:05:23  b\"  Coal is dying due to nat gas fracking. It'...  b\" coal dying nat gas fracking. it's basically...            [coal, dy, nat, ga, frack, basic, dead]\n",
       "...                  ...                  ...                                                ...                                                ...                                                ...\n",
       "2814  142881284019060736  2011-12-03 08:22:07                      was a total non sequitur btw'                            total non sequitur btw'                        [total, non, sequitur, btw]\n",
       "2815  142880871391838208  2011-12-03 08:20:28   Voltaire quote, arguably better than Twain. H...  voltaire quote, arguably better twain. hearing...  [voltair, quot, arguabl, better, twain, hear, ...\n",
       "2816  142188458125963264  2011-12-01 10:29:04   made the volume on the Model S  go to 11.  No...  volume model s 11. need work miniature stonehe...    [volum, model, need, work, miniatur, stoneheng]\n",
       "2817  142179928203460608  2011-12-01 09:55:11   to Iceland on Sat to ride bumper cars on ice!...  iceland sat ride bumper cars ice! no, country,...  [iceland, sat, ride, bumper, car, ic, countri,...\n",
       "2818         15434727182  2010-06-04 18:31:57   ignore prior tweets, as that was someone pret...   ignore prior tweets, pretending :) actually me.'             [ignor, prior, tweet, pretend, actual]\n",
       "\n",
       "[2819 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pre processing (remove stop word and tokenization) of the dataset\n",
    "\n",
    "\n",
    "# Function to remove usernames and links\n",
    "def remove_usernames_links(tweet):\n",
    "    tweet = re.sub('@[^\\s]+','',tweet)\n",
    "    tweet = re.sub('http[^\\s]+','',tweet)\n",
    "    tweet =re.sub(\"b'[^\\s]+\",'', tweet)\n",
    "    tweet =re.sub('b\"[^\\s]+','', tweet)\n",
    "    return tweet\n",
    "data['text'] = data['text'].apply(remove_usernames_links)\n",
    "\n",
    "#Function to remove emoticon\n",
    "def remove_emoticon(tweet):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    tweet = re.sub(emoji_pattern,'',tweet)\n",
    "    return tweet\n",
    "    \n",
    "data['text'] = data['text'].apply(remove_emoticon)\n",
    "\n",
    "\n",
    "# Remove stopword and basic preprocessing\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string\n",
    "# Tokenize the text column to get the new column 'tokenized_text'\n",
    "data['stop'] = data['text'].apply(lambda x : remove_stopwords(x.lower())) \n",
    "data['tokenized'] = data['stop'].apply(lambda x : preprocess_string(x.lower())) \n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "display(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import glove pre trained glove model with twitter\n",
    "import gensim.downloader as api\n",
    "wv = api.load('glove-twitter-100')\n",
    "# Here we take one of the least performant GloVe model for twitter since my computer processor is pretty old (2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oub/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    }
   ],
   "source": [
    "# Vectorization of the tokenized sentences of the dataset with the GloVe model\n",
    "def vectorize(tokenized_sentence):\n",
    "    result = []\n",
    "    for token in tokenized_sentence:\n",
    "        if(token in wv):\n",
    "            result.append(wv[token])\n",
    "    return np.mean(result, axis=0)\n",
    "\n",
    "\n",
    "data['vectorized'] = data['tokenized'].apply(vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            0\n",
       "created_at    0\n",
       "text          0\n",
       "stop          0\n",
       "tokenized     0\n",
       "vectorized    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove null values of the dataset with vectorized\n",
    "data2 =data.dropna()\n",
    "data2.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Apply word embedding to the pre-processed tweets, using the GloVe model (choose the appropriate pre-trained model from here:  https://nlp.stanford.edu/projects/glove/  that conforms with your computer's processor capabilities;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Apply word embeddings to the search query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search query example\n",
    "search= \"Hoping for summer this year\"\n",
    "\n",
    "def vectorize(tokenized_sentence):\n",
    "    result = []\n",
    "    for token in tokenized_sentence:\n",
    "        if(token in wv):\n",
    "            result.append(wv[token])\n",
    "    return np.mean(result, axis=0)\n",
    "\n",
    "\n",
    "vectorized = vectorize(search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.20545775 -0.00383064 -0.01100486 -0.1789822  -0.07985076  0.23289976\n",
      " -0.33620736  0.06706404 -0.28886515  0.2869019  -0.03734813 -0.11326599\n",
      " -3.1475725   0.07909384 -0.14708735 -0.51196355 -0.45266637 -0.12161336\n",
      " -0.41320288  0.66862273 -0.09175472 -0.00653237  0.29945067  0.20366755\n",
      "  0.36769807 -2.4995997  -0.20349513 -0.06186694  0.52953076  0.49043703\n",
      " -0.20725296 -0.26491967 -0.04582856 -0.01996399  0.85011727 -0.07256527\n",
      " -0.19717656 -0.16653176 -0.02182863  0.14568529 -1.7785684   0.21367615\n",
      "  0.18615288 -0.58276385 -0.07617687 -0.10680218  0.08381936 -0.6144123\n",
      " -0.18013899 -0.70793456  0.00954687 -0.11328564  0.02801714  0.12264876\n",
      "  0.23909865  0.09296556  0.03435346 -0.27475414  0.41803887 -0.3336523\n",
      " -0.24002095 -0.18870139  0.13602425  0.15005605  0.02807633  0.06457825\n",
      " -0.14292896  0.201437   -0.01321632 -0.035417    0.26779726 -0.7296068\n",
      "  0.1128322   0.08366135 -0.35698634  0.24036841 -0.24166612  0.5388846\n",
      " -0.24115953 -0.48012257  1.0122213   0.22310232 -0.06033079  0.03726505\n",
      "  0.09827948  0.6282121  -0.60656095  0.03478792  0.10219136  0.5306323\n",
      "  0.18851653 -0.28346014 -0.0914757   0.10570187  0.26897922 -0.49416086\n",
      " -0.4771656  -0.02919273  0.3157221  -0.36915514]\n"
     ]
    }
   ],
   "source": [
    "# The value of vectorized query\n",
    "print(vectorized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Calculate the distance between the embeddings of the search query and that of all the tweets, sort them in increasing order (smaller distance = more relevant to the search query). You will use 2 distance algorithms: cosine similarity and Euclidean distance.\n",
    "### After that, you return the top 5 tweets using each of the aforementioned search algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Euclidian Distance : 7.089881896972656\n",
      "Smallest value 1:  3.852443218231201\n",
      "Smallest value 2:  3.860917091369629\n",
      "Smallest value 3:  3.894739866256714\n",
      "Smallest value 4:  3.9158194065093994\n",
      "Smallest value 5:  3.916928768157959\n"
     ]
    }
   ],
   "source": [
    "# Euclidian Distance  \n",
    "\n",
    "from scipy.spatial import distance\n",
    "# Random example\n",
    "print('Random Euclidian Distance :', distance.euclidean(vectorized,data['vectorized'][2]))\n",
    "\n",
    "def distance_euclidian(datatest): \n",
    "    lowest_euclid = []\n",
    "    for i in datatest['vectorized'] :\n",
    "        if (np.isnan(distance.euclidean(vectorized,i)) == False & np.isinf(distance.euclidean(vectorized,i)) == False):\n",
    "            lowest_euclid.append(distance.euclidean(vectorized,i))\n",
    "    lowest_euclid.sort()\n",
    "    print(\"Smallest value 1: \" , lowest_euclid[0])\n",
    "    print(\"Smallest value 2: \" , lowest_euclid[1])\n",
    "    print(\"Smallest value 3: \" , lowest_euclid[2])\n",
    "    print(\"Smallest value 4: \" , lowest_euclid[3])\n",
    "    print(\"Smallest value 5: \" , lowest_euclid[4])\n",
    "    return lowest_euclid\n",
    "lowest_euclid =distance_euclidian(data2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.39842054\n",
      "Smallest value 1:  0.7182594\n",
      "Smallest value 2:  0.71154684\n",
      "Smallest value 3:  0.70909643\n",
      "Smallest value 4:  0.70170027\n",
      "Smallest value 5:  0.693017\n"
     ]
    }
   ],
   "source": [
    "# Cosie similarity \n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "#Random example \n",
    "cosine = np.dot(vectorized,data['vectorized'][0])/(norm(vectorized)*norm(data['vectorized'][0]))\n",
    "print(\"Cosine Similarity:\", cosine)\n",
    "def distance_cosine(datatest):\n",
    "    biggest_cosine = []\n",
    "    for i in datatest['vectorized'] :\n",
    "        if (np.isnan(np.dot(vectorized,i)/(norm(vectorized)*norm(i))) == False & np.isinf(np.dot(vectorized,i)/(norm(vectorized)*norm(i)))):\n",
    "            biggest_cosine.append(np.dot(vectorized,i)/(norm(vectorized)*norm(i)))\n",
    "    biggest_cosine.sort()\n",
    "\n",
    "    # We remove negative values\n",
    "    def removeNegative(arr):\n",
    "        newArr = []\n",
    "    \n",
    "        for x in range(0, len(arr)):\n",
    "            if (arr[x] >= 0):\n",
    "                newArr.append(arr[x])\n",
    "        return newArr\n",
    "\n",
    "# We take the closest value of one (which mean its similar to our query search)\n",
    "    biggest_cosine = removeNegative(biggest_cosine)\n",
    "    print(\"Biggest value 1: \" , biggest_cosine[-1])\n",
    "    print(\"Biggest value 2: \" , biggest_cosine[-2])\n",
    "    print(\"Biggest value 3: \" , biggest_cosine[-3])\n",
    "    print(\"Biggest value 4: \" , biggest_cosine[-4])\n",
    "    print(\"Biggest value 5: \" , biggest_cosine[-5])\n",
    "    return biggest_cosine\n",
    "biggest_cosine= distance_cosine(data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top tweet    \"Model X has proved both extraordinary and oddly unremarkable. Rating: \\xe2\\x98\\x85\\xe2\\x98\\x85\\xe2\\x98\\x85\\xe2\\x98\\x85\\xe2\\x98\\x85\"   \n",
      "Top tweet  b'  Yes, if the trend continues, before'\n",
      "Top tweet  b' Nice ride! Looking forward to seeing you tomorrow.'\n",
      "Top tweet   3 design sketches \n",
      "Top tweet  b' Will do. Probably at least a few days. Depends on whether we need to pull the turbopumps.'\n"
     ]
    }
   ],
   "source": [
    "liste_euc = [lowest_euclid[0],lowest_euclid[1],lowest_euclid[2],lowest_euclid[3],lowest_euclid[4]]\n",
    "\n",
    "def top_5_tweets_eucli(liste_distance_euclid):\n",
    "    d =0\n",
    "    for distance_euclidian in liste_distance_euclid:\n",
    "        for i in data2['vectorized'] :\n",
    "            d+= 1\n",
    "            if distance.euclidean(vectorized,i) == distance_euclidian :\n",
    "                print(\"Top tweet \",data['text'][d])\n",
    "        d= 0\n",
    "top_5_tweets_eucli(liste_euc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top tweet   to help Boeing is real &amp; am corresponding w 787 chief engineer. Junod's Esquire article had high fiction content.\"\n",
      "Top tweet    Dragon on Mars. \n",
      "Top tweet   2008 meltdown, vacation for me just meant email with a view, but SpaceX &amp; Tesla are now strong enough that I can make it real (yay!!)'\n",
      "Top tweet   near term plans to IPO  Only possible in very long term when Mars Colonial Transporter is flying regularly.'\n",
      "Top tweet   when self-driving cars become safer than human-driven cars, the public may outlaw the latter. Hopefully not.'\n"
     ]
    }
   ],
   "source": [
    "liste_cos = [biggest_cosine[0],biggest_cosine[1],biggest_cosine[2],biggest_cosine[3],biggest_cosine[4]]\n",
    "\n",
    "def top_5_tweets_cosi(liste_distance_cosine):\n",
    "    d =0\n",
    "    for distance_euclidian in liste_distance_cosine:\n",
    "        for i in data2['vectorized'] :\n",
    "            d+= 1\n",
    "            if np.dot(vectorized,i)/(norm(vectorized)*norm(i)) == distance_euclidian :\n",
    "                print(\"Top tweet \",data['text'][d])\n",
    "        d= 0\n",
    "top_5_tweets_cosi(liste_cos)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b5b270ea6e464d855d17abd4fc71c5752b3009b37682238331a225d42ee4ac9e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
